---
title: "Pore size and Coordination number analysis for Beleyur et al 2020, *The web architecture, dynamics, and silk investment in the social spider, Stegodyphus sarasinorum*"
author: "Thejasvi Beleyur"
date: "2020-03-08"
output: html_document
---

*This is one of three Rmd files accompanying the manuscript. The other file is called 'beleyur_et_al_silk_analyses.Rmd' (analyses the silk length on the web and the per-capita silk investment.') and 'weight_loss_analysis.Rmd' (analyses weight loss of individuals across colonies over the experiment)*

##### You are reading a version of the report compiled on `r date()`

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Pore size and Coordination number
This notebook will visualise image analysis data from the Murthy lab. Saurabh looked at the pore sizes and coordination numbers of the stego webs across group size and time. To ensure the plots in the paper look the same - I will be re-plotting the data in ggplot2, and try to  perform the same nparLD analyses
on the data as I did for the previous data I extracted (total silk length , density). 

### Visualising the data
```{r loading data}
library(readxl)
library(ggplot2)
library(stringr)
library(R.matlab)
data_file <- file.path('pore_size_and_coordination_number_data','Results.xlsx')

# each web's mean pore size + coordination number data is saved as a separate sheet
sheet_names <- excel_sheets(data_file)
sheet_names

# load one example dataset and create a plot of the pore size (PS) and coordination number (CN)
for(example_sheet in sheet_names)
{
example_sheet
eg_data <- read_excel(data_file, sheet=example_sheet)

# plot raw data for Coordination number
cn_days<-ggplot(eg_data, aes(Days, CN) )+geom_point()+geom_line() +ylab('Coordination number') + xlab('Days') + ggtitle(sprintf('raw plot for %s',example_sheet))+ylim(0,10)
print(cn_days)

poresize_days<-ggplot(eg_data, aes(Days, Pores) )+geom_point()+geom_line() +ylab('Pore size') + xlab('Days') + ggtitle(sprintf('raw plot for %s',example_sheet))+ylim(0,2000)
print(poresize_days)


}

```
### 2020-03-08: Something odd with 1C:
I just noticed that there's something a bit odd with 1C's first and last coordination values. Both the first and last values are 1. I'm guessing a coordination number of 1 means that there's no strings crossing -- or that there's 2 of them at a junction. Either way the value of 1 is odd/unlikely because the remaining values are at ~2.5, and then on the last day, the coordination number drops to 1. The drop implies a sudden drastic change in structure - which is not seen in the images. I'm guessing this  is a typo or an issue with the noisy  image interacting with the code.  Have written to Saurabh. If the images aren't clear, the data points could either be removed or corrected. Am waiting for a clarification - until then , however, I'm still going to get the code ready. 

### 2020-04-07: Updates and responses from Saurabh

#### 1. Weird trapezoidal pattern in 1C (up, flat and then down back to starting value)
I'd written to Saurabh about the weird data patterns in 1C, and he's replied just about  month ago now. It seems to be an issue with how the images were numbered. 

*"The confusion in 1c is caused because of the wrong numbering of original images. The original images are attached in images.zip. You can see for 1c, the web is fully developed for image 1 and least for 91. I had changed the numbering (in reverse order). So the first image does not have any pore or closed polygon which means the pore area can not be calculated. The coordination number of 1, on the other hand, comes from the two segregated silk strands on the left and topside. Further, there are only 10 images so the 11 entry of coordination number 1 is an error. This error happened in renumbering the image. So, you can remove the 11th entry."*


#### 2. Using the raw data instead of mean values
I'd noticed that using the mean value per web per day basically meant that outliers had a particularly strong effect 
on the trends. I'd asked Saurabh to send the original values to play around with. He has sent them in the folder titled *ImagesWithResults.zip* on 2020-03-09. The zipped folder has one subfolder for each web. Within this subfolder, there is a results file with a series of probe images with the centre of each cell plotted over it. 

#### 3. Using a common 'probe size' across $\geq 5$ group sizes
The previous dataset had used a small probe size for single spiders (125x125), and a medium probe size for 
group sizes 5 & 10 (300x300) and a large probe size for 25 spiders (500x500). I then asked Saurabh to re-analyse
the 25 spider group with a probe size of 300x300 pixels to avoid sampling biases. He has sent the results for 
25 spider webs with 300x300 probe sizes too in the file *"Results.xlsx"* on 2020-03-15 11:09 CET. I've renamed this file to *"Saurabh_300x300_probe_area_groupsize_Results.xlsx"*. 


```{r loading latest data from Results.xlsx}
all_web_datafile <- 'Saurabh_300x300_probe_area_groupsize_Results.xlsx'
sheet_names<-excel_sheets(all_web_datafile)

web_data_300300 <- data.frame()
for (each_sheet in sheet_names){
  
  df<-read_excel(all_web_datafile, sheet=each_sheet)
  group_size <- as.numeric(substr(each_sheet, 1, nchar(each_sheet)-1))
  df$group_size <- group_size
  df$web_id <- each_sheet
  web_data_300300 <- rbind(web_data_300300, df)
  
  
}


```



### Combining all web data into one dataframe

```{r combining data}

all_web_data_raw <- data.frame()

for(sheet in sheet_names)
{
sheet
group_size <- as.numeric(substr(sheet, 1, nchar(sheet)-1))

one_web_data <- read_excel(data_file, sheet=sheet)
one_web_data$group_size <- group_size
one_web_data$web_id <- sheet
all_web_data_raw <- rbind(all_web_data_raw,one_web_data)
}

```


#### Are they actually the same dataset?
It seems like the datapoints in the two xlsx are actually the same (except for the data in 1C), is this really so?

```{r checking dataset similarity}

sheet_data_is_the_same <- data.frame()
for (sheet in sheet_names){
  old_dataset <- subset(all_web_data_raw, web_id==sheet)
  new_dataset <- subset(web_data_300300, web_id==sheet)
  dataset_the_same <- identical(old_dataset, new_dataset)
  current_results<- data.frame(row.names=1)
  current_results$dataset_the_same <- dataset_the_same
  current_results$webid <- sheet
  sheet_data_is_the_same <- rbind(sheet_data_is_the_same, current_results)
}

print(sheet_data_is_the_same)
```

#### 2020-04-08 : It seems like the two 'Results.xlsx' are basically the same, will need to hear back from Saurabh.


### Removing the excess spider colonies
Remove some of the colonies that slipped through in the last round of analysis 

```{r}

excess_spiders <- read.csv('excess_spiders.csv')
colonies_to_remove <- subset(excess_spiders, excess_spiders_observed==TRUE)$colonyname
#load function which removes all rows in the colonynames list/vector
source('remove_colonies.R')
all_web_data<- remove_colonies(all_web_data_raw, colonies_to_remove,'web_id')

```

### Plotting the long format dataset

```{r plotting the combined dataset}
pd <- position_dodge(width = 0.8)
poresize_days<-ggplot(all_web_data, aes(x=as.factor(Days), y=Pores, shape=as.factor(group_size), group=web_id)) + geom_point(position=pd)+geom_line(linetype='solid', size=0.75, position=pd) +ylab('Pore size') + xlab('Days')  
poresize_days


```


```{r making same format}
# pore size which mimics layout of Figure 3 in manuscript ( length of silk vs days acros groupsizes)

# now need to make a new plot with days on x , mean + se on y axis for each group size :
poresize_mean <- aggregate(Pores ~ Days*group_size, data = all_web_data, mean, na.action=na.omit); 
colnames(poresize_mean)[3] <- 'mean_poresize'
poresize_sd <-aggregate(Pores ~ Days*group_size, data = all_web_data, sd, na.action=na.omit);
colnames(poresize_sd)[3] <- 'sd_poresize'

poresize_mean_sd<-merge(poresize_mean,poresize_sd); 
poresize_mean_sd<-poresize_mean_sd[order(poresize_mean_sd[,1],poresize_mean_sd[,2]),]

#
pd <- position_dodge(width = 0.8)
poresize_data<-subset(poresize_mean_sd,Days<8)


mgplt<- ggplot(poresize_data,aes(x=as.factor(Days),
                                 y=mean_poresize,
                                 shape=as.factor(group_size),
                                 group=as.factor(group_size)))+geom_pointrange(aes(ymin=mean_poresize-sd_poresize, ymax=mean_poresize+sd_poresize,width=0.9),size=1.3,position=pd) +scale_shape_manual(values=c(15,16,17,18))+ xlab('Days') + ylab('Pore size (a.u.)') +geom_line(lwd=1,position=pd)+labs(shape = "Group Size")+ theme(text = element_text(size=15));
mgplt



```

```{r plotting coordination number}
# now need to make a new plot with days on x , mean + se on y axis for each group size :
coodnum_mean <- aggregate(CN ~ Days*group_size, data = all_web_data, mean, na.action=na.omit); 
colnames(coodnum_mean)[3] <- 'mean_coodnum'
coodnum_sd <-aggregate(CN ~ Days*group_size, data = all_web_data, sd, na.action=na.omit);
colnames(coodnum_sd)[3] <- 'sd_coodnum'

coodnum_mean_sd<-merge(coodnum_mean,coodnum_sd); 
coodnum_mean_sd<-coodnum_mean_sd[order(coodnum_mean_sd[,1],coodnum_mean_sd[,2]),]

#
coodnum_data<-subset(coodnum_mean_sd,Days<8)


mgplt2<- ggplot(coodnum_data,aes(x=as.factor(Days),
                                 y=mean_coodnum,
                                 shape=as.factor(group_size),
                                 group=as.factor(group_size)))+geom_pointrange(aes(ymin=mean_coodnum-sd_coodnum, ymax=mean_coodnum+sd_coodnum,width=0.9),size=1.3,position=pd) +scale_shape_manual(values=c(15,16,17,18))+ xlab('Days') + ylab('Coordination nummber') +geom_line(lwd=1,position=pd)+labs(shape = "Group Size")+ theme(text = element_text(size=15))+ylim(1,4);
mgplt2



```


### Mean CN and pore size of all webs on day 7 
```{r}
# Mean coordination number on day 7 across group sizes
print(subset(coodnum_data,Days==7))

# Mean pore size on day 7 across group sizes
print(subset(poresize_data,Days==7))


```



#### What remains to be done:

* Perform the nparLD analysis on the pore size and coordination number values and see if they differ across group size
* Do the results change a lot if you  use the mean values or the median values? (Ideally I'd use the median values, but 
right now I realise it might take a bit more time to figure  out  that part!)


### nparLD on the mean CN and pore-size

```{r }

porsize_data = all_web_data[,c('Pores', 'Days','web_id','group_size')]
porsizedata_r <- subset(porsize_data,  Days<6)
colnames(porsizedata_r) <- c('poresize','days','webid','groupsize')

# 1C has a missing entry because there are polygons in the images. 
# I'm replacing this to mean a pore size of 0 on day 1
print(porsizedata_r[is.na(porsizedata_r$poresize),])

if (sum(is.na(porsizedata_r$poresize))==1){
  porsizedata_r[is.na(porsizedata_r$poresize),]$poresize <- 0
}

# order rows by the days column
porsizedata<-porsizedata_r[order(porsizedata_r$days,-porsizedata_r$groupsize),]
porsizedata$webidnum <- as.numeric(as.factor(porsizedata$webid))
# save and run this part in R 2.15.3
write.csv(porsizedata,'poresize_nparLD.csv') # this dataset is loaded and run by 'poresize_CN_nparLD.R'

#### Now checkout the coordination number
cn_data_r = all_web_data[,c('CN', 'Days','web_id','group_size')]
colnames(cn_data_r) <- c('cn','days','webid','groupsize')
cn_data <- subset(cn_data_r,  days<6)
cn_data <-cn_data[order(porsizedata_r$days,-porsizedata_r$groupsize),]
cn_data$webidnum <- as.numeric(as.factor(cn_data$webid))
write.csv(cn_data, 'cn_nparLD.csv')
```


#### How to use Renv:

#### The 'interactive way'
Run ```source ~/.bash_profile``` to activate the ```Renv``` command. Type ```Renv``` . 
To start an R session just type ```R``` and the chosen version should start. Within the R session 
run the command ```source('poresize_CN_nparLD.R') ``` to get the nparLD results as run on R version 2.15.3

#### The 'automated way'

```{r}
# this activates Renv. Make sure the default R version is set to 2.15.3
# this is a Linux specific way to do it 
system('bash -c "source ~/.bash_profile"')
```

### Are the mean-based nparLD affected by outliers
2020-06-08
After D & H discussed the results, they suspect the Groupsize 1 may actually be contributing most of the 
variation to the whole dataset, and thus contribute to the somewhat unexpected results. Either way, even before
the results too I realised using the mean +/- sd wasn't the best way to move forward because I too suspected there'd be lots of outlier effects on the results. Anyway, now I'm going to load up the cell/polygon level data and 
get the *median* stats for each web on each day. Here it goes. 

#### Loading data from individual .mat files:
The results within each web image subfolder seem to be in the .mat files. I now have to figure out how to get the data from the .mat file into an R compatible. There seems to be a couple of options: 1) [R.matlab](https://www.rdocumentation.org/packages/R.matlab/versions/3.6.2/topics/readMat) or [rmatio](https://www.rdocumentation.org/packages/rmatio/versions/0.14.0/topics/read.mat). It seems like R.matlab is the more mature one so I'll use R.matlab!

```{r loading .mat files}
web_folders = list.files('ImagesWithResults/', include.dirs = TRUE)
# function to get all MAT files in a folder
get_all_MAT_files <- function(directory){
    mat_files = list.files(directory,pattern='*.mat')
    mat_files
}

read_MAT_file <- function(mat_file){
  data<-readMat(mat_file)
  data}

# function which calculates median coordination number for a web's MAT file
get_median_CN <- function(mat_data){
                all_cns <- mat_data$CN
                median_cn <- median(all_cns)
                median_cn
}


flatten_and_median<-function(imgreg)
      {
    pore_size <- imgreg[1,,1]
    poresize_flattened<- vector()
    for (each in imgreg[1,,1]){
      poresize_flattened <- c(each, poresize_flattened)
    }
    median_poresize <- median(poresize_flattened)
    median_poresize
      }

get_median_poresize<-function(mat_data){
              imgreg<-mat_data$imgRegProps
              median_poresize<-tryCatch(flatten_and_median(imgreg),
                                        error=function(imgreg){NA})
              median_poresize
                            }

load_and_get_median_data <- function(mat_file)
  {
  mat_data<-read_MAT_file(mat_file)
  median_CN <- get_median_CN(mat_data)
  median_poresize <- get_median_poresize(mat_data)
  median_data<-c(median_CN, median_poresize)
  median_data
  }

```

### Trying to decipher the structure of one of the .mat files

```{r opening a .mat file}
remove_last_4_chars<-function(some_string){str_sub(some_string, 1, str_length(some_string)-4)}

all_median_data <- c(NA, NA, NA, NA)
for (each_web in web_folders){
  source_folder = paste('ImagesWithResults/',each_web,'/Results/',sep='')
  web_mat_files <- list.files(source_folder, pattern='*.mat')
  for (each_mat in web_mat_files){
    full_matfile_path = paste(source_folder,each_mat, sep='')
    median_data <- load_and_get_median_data(full_matfile_path)
    day_number <- as.numeric(remove_last_4_chars(each_mat))
    all_median_data <- rbind(all_median_data, c(median_data, each_web, day_number))
                                 }
                                }

median_df <- data.frame(all_median_data)
colnames(median_df) <- c('median_CN','median_poresize','web_id','day')
median_df <- median_df[2:nrow(median_df),]
median_df$median_poresize <- as.numeric(median_df$median_poresize)
median_df$median_CN <- as.numeric(median_df$median_CN)

# save the median data to csv so that it can be loaded and run for the nparLD analysis
median_df$group_size <- as.numeric(str_sub(median_df$web_id, 1, str_length(median_df$web_id)-1))

# insert entry for day 1 of 1B - a web which didn't show any developement and thus have missing day1 entry
# these NA's should actually be zeros. 
day1_1B <- data.frame(median_CN=0, median_poresize=0, web_id='1B', day=1, group_size=1)
median_df <- rbind(median_df, day1_1B)

# 1C only showed a strand, and thus had a coordination number, but no poresize
day1_row = which(median_df$web_id=='1C' & median_df$day==1)
median_df[day1_row,]$median_poresize = 0

# remove the excess spider colonies here too
median_df <- remove_colonies(median_df, colonies_to_remove, 'web_id')

write.csv(median_df, 'median_poresize_cn.csv')
```
### Run the nparLD analysis using R version 2.15.3

```{r}

# call the script from the command line and generate the results 
system('bash -c "Rscript poresize_CN_nparLD.R"')

```


### Analyses outputs to be run with nparLD:
```{r}
# now load the nparLD data
load('results_CNandPoresize_nparld.RData') # raw mean analysis
load('results_CNandPoresize_woday1_nparld.RData') # without day 1 analysis
load('results_CNandPoresize_wo1spider_nparld.RData') # without 1 spider analysis
load('results_MEDIAN_CNandPoresize_nparld.RData') # median data analysis 
```
#### Raw mean data (till day5):

```{r}
print('CN mean')
print(res_cn$ANOVA.test)

print('Poresize mean')
print(res_poresize$ANOVA.test)

```
It can be seen that group size is significant, which really goes against the observations in the data. This could be 
a result of 3 things : 1) the means are sensitive to outliers and thus create extra variation 2) spiders on their first
day show more variable web building behaviour and thus create variation, 3) single spiders themselves in general add to the variation. 

* mean analysis (a) : remove day 1 to see if all groups are indistinguishable
```{r}
print('CN mean, without day1')
print(woday1_cn_results$ANOVA.test)

print('Poresize mean, without day 1')
print(woday1_poresize_results$ANOVA.test)

```

* mean analysis (b) : remove single spiders to see if all groups are indistinguishable 
```{r}

print('CN mean, without single spiders')
print(wo1spider_res$ANOVA.test)

print('Poresize mean, without single spiders')
print(wo1spider_poresize_results$ANOVA.test)

```

* median analysis : just use the *median* measurements of each web on each day and check the overall results 
```{r}

print('CN Median')
print(results_median_CN$ANOVA.test)

print('Poresize Median')
print(results_med_poresize$ANOVA.test)


```
### Plotting median data
Let's also plot the median CN and pore size data to visualise how similar/different they are 

```{r plotting median data}
med_plot<-ggplot(median_df,aes(x=day,y=median_CN,group=group_size,color=as.factor(group_size)))+stat_summary(fun.ymin = min,fun.ymax =max ,size=1.5,position=position_dodge(width=0.65))
med_plot

pd <- position_dodge(width = 0.8)
median_df <- subset(median_df, day!=91)
median_df$dayfactor <- factor(median_df$day, levels=c('1','2','3','4','5','6','7','8','9','10'))

# median pore size plot with min and max
mgplt<- ggplot(median_df,aes(x=dayfactor,y=median_CN, shape=as.factor(group_size), group=as.factor(group_size))) +geom_pointrange(aes(ymin=min(median_CN), ymax=max(median_CN),width=0.9),size=1.3,position=pd) +scale_shape_manual(values=c(15,16,17,18))+ xlab('Days') + ylab('Median Coordination number') +geom_line(lwd=1,position=pd)+labs(shape = "Group Size")+ theme(text = element_text(size=15))  ;
mgplt


mgplt2<- ggplot(median_df,aes(x=dayfactor,y=median_poresize, shape=as.factor(group_size), group=as.factor(group_size))) +geom_pointrange(aes(ymin=min(median_poresize), ymax=max(median_poresize),width=0.9),size=1.3,position=pd) +scale_shape_manual(values=c(15,16,17,18))+ xlab('Days') + ylab('Median poresize') +geom_line(lwd=1,position=pd)+labs(shape = "Group Size")+ theme(text = element_text(size=15))  ;
mgplt2

```


### The R packages and R version required to run this analysis 

```{r}
# code from 
# https://www.r-bloggers.com/list-of-user-installed-r-packages-and-their-versions/

ip <- as.data.frame(installed.packages()[,c(1,3:4)])
rownames(ip) <- NULL
ip <- ip[is.na(ip$Priority),1:2,drop=FALSE]
print(ip, row.names=FALSE)

## The R version used to run this analysis 
print(R.version)
```



```{r run_time, echo=FALSE}
print(sprintf('This notebook was last run on: %s' ,Sys.time()))
```

