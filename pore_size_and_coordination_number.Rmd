---
title: "Pore size and Coordination number"
author: "Thejasvi Beleyur"
date: "2020-03-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Pore size and Coordination number
This notebook will visualise image analysis data from the Murthy lab. Saurabh looked at the pore sizes and coordination numbers of the stego webs across group size and time. To ensure the plots in the paper look the same - I will be re-plotting the data in ggplot2, and try to  perform the same nparLD analyses
on the data as I did for the previous data I extracted (total silk length , density). 

### Visualising the data
```{r loading data}
library(readxl)
library(ggplot2)
library(stringr)
library(R.matlab)
data_file <- file.path('pore_size_and_coordination_number_data','Results.xlsx')

# each web's mean pore size + coordination number data is saved as a separate sheet
sheet_names <- excel_sheets(data_file)
sheet_names

# load one example dataset and create a plot of the pore size (PS) and coordination number (CN)
for(example_sheet in sheet_names)
{
example_sheet
eg_data <- read_excel(data_file, sheet=example_sheet)

# plot raw data for Coordination number
cn_days<-ggplot(eg_data, aes(Days, CN) )+geom_point()+geom_line() +ylab('Coordination number') + xlab('Days') + ggtitle(sprintf('raw plot for %s',example_sheet))+ylim(0,10)
print(cn_days)

poresize_days<-ggplot(eg_data, aes(Days, Pores) )+geom_point()+geom_line() +ylab('Pore size') + xlab('Days') + ggtitle(sprintf('raw plot for %s',example_sheet))+ylim(0,2000)
print(poresize_days)


}

```
### 2020-03-08: Something odd with 1C:
I just noticed that there's something a bit odd with 1C's first and last coordination values. Both the first and last values are 1. I'm guessing a coordination number of 1 means that there's no strings crossing -- or that there's 2 of them at a junction. Either way the value of 1 is odd/unlikely because the remaining values are at ~2.5, and then on the last day, the coordination number drops to 1. The drop implies a sudden drastic change in structure - which is not seen in the images. I'm guessing this  is a typo or an issue with the noisy  image interacting with the code.  Have written to Saurabh. If the images aren't clear, the data points could either be removed or corrected. Am waiting for a clarification - until then , however, I'm still going to get the code ready. 

### 2020-04-07: Updates and responses from Saurabh

#### 1. Weird trapezoidal pattern in 1C (up, flat and then down back to starting value)
I'd written to Saurabh about the weird data patterns in 1C, and he's replied just about  month ago now. It seems to be an issue with how the images were numbered. 

*"The confusion in 1c is caused because of the wrong numbering of original images. The original images are attached in images.zip. You can see for 1c, the web is fully developed for image 1 and least for 91. I had changed the numbering (in reverse order). So the first image does not have any pore or closed polygon which means the pore area can not be calculated. The coordination number of 1, on the other hand, comes from the two segregated silk strands on the left and topside. Further, there are only 10 images so the 11 entry of coordination number 1 is an error. This error happened in renumbering the image. So, you can remove the 11th entry."*


#### 2. Using the raw data instead of mean values
I'd noticed that using the mean value per web per day basically meant that outliers had a particularly strong effect 
on the trends. I'd asked Saurabh to send the original values to play around with. He has sent them in the folder titled *ImagesWithResults.zip* on 2020-03-09. The zipped folder has one subfolder for each web. Within this subfolder, there is a results file with a series of probe images with the centre of each cell plotted over it. 

#### 3. Using a common 'probe size' across $\geq 5$ group sizes
The previous dataset had used a small probe size for single spiders (125x125), and a medium probe size for 
group sizes 5 & 10 (300x300) and a large probe size for 25 spiders (500x500). I then asked Saurabh to re-analyse
the 25 spider group with a probe size of 300x300 pixels to avoid sampling biases. He has sent the results for 
25 spider webs with 300x300 probe sizes too in the file *"Results.xlsx"* on 2020-03-15 11:09 CET. I've renamed this file to *"Saurabh_300x300_probe_area_groupsize_Results.xlsx"*. 


```{r loading latest data from Results.xlsx}
all_web_datafile <- 'Saurabh_300x300_probe_area_groupsize_Results.xlsx'
sheet_names<-excel_sheets(all_web_datafile)

web_data_300300 <- data.frame()
for (each_sheet in sheet_names){
  
  df<-read_excel(all_web_datafile, sheet=each_sheet)
  group_size <- as.numeric(substr(each_sheet, 1, nchar(each_sheet)-1))
  df$group_size <- group_size
  df$web_id <- each_sheet
  web_data_300300 <- rbind(web_data_300300, df)
  
  
}


```




### Combining all web data into one dataframe

```{r combining data}

all_web_data <- data.frame()

for(sheet in sheet_names)
{
sheet
group_size <- as.numeric(substr(sheet, 1, nchar(sheet)-1))

one_web_data <- read_excel(data_file, sheet=sheet)
one_web_data$group_size <- group_size
one_web_data$web_id <- sheet
all_web_data <- rbind(all_web_data,one_web_data)
}

```


#### Are they actually the same dataset?
It seems like the datapoints in the two xlsx are actually the same (except for the data in 1C), is this really so?

```{r checking dataset similarity}

sheet_data_is_the_same <- data.frame()
for (sheet in sheet_names){
  old_dataset <- subset(all_web_data, web_id==sheet)
  new_dataset <- subset(web_data_300300, web_id==sheet)
  dataset_the_same <- identical(old_dataset, new_dataset)
  current_results<- data.frame(row.names=1)
  current_results$dataset_the_same <- dataset_the_same
  current_results$webid <- sheet
  sheet_data_is_the_same <- rbind(sheet_data_is_the_same, current_results)
}

print(sheet_data_is_the_same)
```

#### 2020-04-08 : It seems like the two 'Results.xlsx' are basically the same, will need to hear back from Saurabh.



### Plotting the long format dataset

```{r plotting the combined dataset}
pd <- position_dodge(width = 0.8)
poresize_days<-ggplot(all_web_data, aes(x=as.factor(Days), y=Pores, shape=as.factor(group_size), group=web_id)) + geom_point(position=pd)+geom_line(linetype='solid', size=0.75, position=pd) +ylab('Pore size') + xlab('Days')  
poresize_days


```


```{r making same format}
# pore size which mimics layout of Figure 3 in manuscript ( length of silk vs days acros groupsizes)

# now need to make a new plot with days on x , mean + se on y axis for each group size :
poresize_mean <- aggregate(Pores ~ Days*group_size, data = all_web_data, mean, na.action=na.omit); 
colnames(poresize_mean)[3] <- 'mean_poresize'
poresize_sd <-aggregate(Pores ~ Days*group_size, data = all_web_data, sd, na.action=na.omit);
colnames(poresize_sd)[3] <- 'sd_poresize'

poresize_mean_sd<-merge(poresize_mean,poresize_sd); 
poresize_mean_sd<-poresize_mean_sd[order(poresize_mean_sd[,1],poresize_mean_sd[,2]),]

#
pd <- position_dodge(width = 0.8)
poresize_data<-subset(poresize_mean_sd,Days<8)


mgplt<- ggplot(poresize_data,aes(x=as.factor(Days),
                                 y=mean_poresize,
                                 shape=as.factor(group_size),
                                 group=as.factor(group_size)))+geom_pointrange(aes(ymin=mean_poresize-sd_poresize, ymax=mean_poresize+sd_poresize,width=0.9),size=1.3,position=pd) +scale_shape_manual(values=c(15,16,17,18))+ xlab('Days') + ylab('Pore size (a.u.)') +geom_line(lwd=1,position=pd)+labs(shape = "Group Size")+ theme(text = element_text(size=15));
mgplt



```

```{r plotting coordination number}
# now need to make a new plot with days on x , mean + se on y axis for each group size :
coodnum_mean <- aggregate(CN ~ Days*group_size, data = all_web_data, mean, na.action=na.omit); 
colnames(coodnum_mean)[3] <- 'mean_coodnum'
coodnum_sd <-aggregate(CN ~ Days*group_size, data = all_web_data, sd, na.action=na.omit);
colnames(coodnum_sd)[3] <- 'sd_coodnum'

coodnum_mean_sd<-merge(coodnum_mean,coodnum_sd); 
coodnum_mean_sd<-coodnum_mean_sd[order(coodnum_mean_sd[,1],coodnum_mean_sd[,2]),]

#
coodnum_data<-subset(coodnum_mean_sd,Days<8)


mgplt2<- ggplot(coodnum_data,aes(x=as.factor(Days),
                                 y=mean_coodnum,
                                 shape=as.factor(group_size),
                                 group=as.factor(group_size)))+geom_pointrange(aes(ymin=mean_coodnum-sd_coodnum, ymax=mean_coodnum+sd_coodnum,width=0.9),size=1.3,position=pd) +scale_shape_manual(values=c(15,16,17,18))+ xlab('Days') + ylab('Coordination nummber') +geom_line(lwd=1,position=pd)+labs(shape = "Group Size")+ theme(text = element_text(size=15))+ylim(1,4);
mgplt2



```

#### What remains to be done:

* Perform the nparLD analysis on the pore size and coordination number values and see if they differ across group size
* Do the results change a lot if you  use the mean values or the median values? (Ideally I'd use the median values, but 
right now I realise it might take a bit more time to figure  out  that part!)


### nparLD on the mean CN and pore-size
There's some weird problem that's preventing me from going ahead. 

```{r }

porsize_data = all_web_data[,c('Pores', 'Days','web_id','group_size')]
porsizedata_r <- subset(porsize_data,  Days<6)
colnames(porsizedata_r) <- c('poresize','days','webid','groupsize')

# 1C has a missing entry because there are polygons in the images. 
# I'm replacing this to mean a pore size of 0 on day 1
print(porsizedata_r[is.na(porsizedata_r$poresize),])

if (sum(is.na(porsizedata_r$poresize))==1){
  porsizedata_r[is.na(porsizedata_r$poresize),]$poresize <- 0
}

# order rows by the days column
porsizedata<-porsizedata_r[order(porsizedata_r$days,-porsizedata_r$groupsize),]
porsizedata$webidnum <- as.numeric(as.factor(porsizedata$webid))
# save and run this part in R 2.15.3
write.csv(porsizedata,'poresize_nparLD.csv') # this dataset is loaded and run by 'poresize_CN_nparLD.R'

#### Now checkout the coordination number
cn_data_r = all_web_data[,c('CN', 'Days','web_id','group_size')]
colnames(cn_data_r) <- c('cn','days','webid','groupsize')
cn_data <- subset(cn_data_r,  days<6)
cn_data <-cn_data[order(porsizedata_r$days,-porsizedata_r$groupsize),]
cn_data$webidnum <- as.numeric(as.factor(cn_data$webid))
write.csv(cn_data, 'cn_nparLD.csv')
```



```{r loading}

#### source('poresize_CN_nparLD.R') # - this must be done in an session running 

# now load the nparLD data
load('results_CNandPoresize_nparld.RData')

print(res_cn)

print(res_poresize)

#last run times of the results
format(res_poresize$lastrun, "%a %b %d %X %Y")
format(res_cn$lastrun, "%a %b %d %X %Y")

```



### Are the mean-based nparLD affected by outliers
2020-06-08
After D & H discussed the results, they suspect the Groupsize 1 may actually be contributing most of the 
variation to the whole dataset, and thus contribute to the somewhat unexpected results. Either way, even before
the results too I realised using the mean +/- sd wasn't the best way to move forward because I too suspected there'd be lots of outlier effects on the results. Anyway, now I'm going to load up the cell/polygon level data and 
get the *median* stats for each web on each day. Here it goes. 

#### Loading data from individual .mat files:
The results within each web image subfolder seem to be in the .mat files. I now have to figure out how to get the data from the .mat file into an R compatible. There seems to be a couple of options: 1) [R.matlab](https://www.rdocumentation.org/packages/R.matlab/versions/3.6.2/topics/readMat) or [rmatio](https://www.rdocumentation.org/packages/rmatio/versions/0.14.0/topics/read.mat). It seems like R.matlab is the more mature one so I'll use R.matlab!

```{r loading .mat files}
all_files_in_folder = list.files('ImagesWithResults/', recursive = TRUE, include.dirs = TRUE)

mat_files <- list()
for (each_filepath in all_files_in_folder){
  
  if (str_detect(each_filepath, '.mat')){
    mat_files <- append(mat_files, each_filepath)
  }
  
}


```

### Trying to decipher the structure of one of the .mat files

```{r opening a .mat file}

# function which calculates median coordination number for a web's MAT file
get_median_CN <- function(mat_data){
                all_cns <- mat_file$CN
                median_cn <- median(all_cns)
                median_cn
                  }
get_median_poresize<-function(mat_data){
              imgreg<-mat_data$imgRegProps
              pore_size <- imgreg[1,,1]
              poresize_flattened<- vector()
              for (each in imgreg[1,,1]){
                poresize_flattened <- c(each, poresize_flattened)
              }
              median_poresize <- median(poresize_flattened)
              median_poresize
              }

full_filepath = file.path('ImagesWithResults', mat_files[2])
# print the path of the .mat file being used here
mat_files[2]
d <- readMat(full_filepath) # read the .mat file 
#str(d)


imgreg <- d$imgRegProps

d$centroids
```


```{r run_time, echo=FALSE}
print(sprintf('This notebook was last run on: %s' ,Sys.time()))
```

